{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH2Jxs7W9ZWKWK1OCRpL1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/Sentiment/blob/main/ArEn-TweetSentiment-BERT-Hatem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "!pip install -q datasets transformers evaluate pandas scikit-learn\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import evaluate\n",
        "import os\n",
        "\n",
        "file_path_ar_dataset = \"/content/Arabic Tweets Sentiment Classification 2024/Arabic_tweets_sentiment.csv\"\n",
        "file_path_en_dataset = \"/content/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "if os.path.exists(file_path_ar_dataset):\n",
        "     print(\"âœ… Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯.\")\n",
        "     print(file_path_ar_dataset)\n",
        "     print(\"-----\")else:\n",
        "     print(\"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.\")\n",
        "     !wget -q https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/m88gg52wp7-1.zip -O arabicdata1.zip\n",
        "     !unzip -o arabicdata1.zip\n",
        "\n",
        "\n",
        "# âœ… ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† UCI\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV with the correct separator and rename the column\n",
        "#ds_ar = pd.read_csv(\"/content/Arabic Tweets Sentiment Classification 2024/Arabic_tweets_sentiment.csv\", encoding='utf-8', sep='\\t')\n",
        "ds_ar = pd.read_csv(file_path_ar_dataset, encoding='utf-8', sep='\\t')\n",
        "\n",
        "ds_ar.rename(columns={'class': 'class'}, inplace=True)\n",
        "\n",
        "\n",
        "# ÙÙ„ØªØ±Ø© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ ÙÙ‚Ø· Ø¹Ù„Ù‰ Positive Ø£Ùˆ Negative\n",
        "ds_ar = ds_ar[ds_ar[\"class\"].isin([\"Positive\", \"Negative\"])]\n",
        "\n",
        "\n",
        "# ğŸ§¼ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©\n",
        "def clean_english_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)     # remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)        # remove mentions\n",
        "    text = re.sub(r\"#\", \"\", text)           # remove hashtags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)     # remove punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)         # remove numbers\n",
        "    return text.lower().strip()\n",
        "\n",
        "# ğŸ§¼ ØªØ·Ø¨ÙŠØ¹ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
        "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
        "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
        "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
        "    return text\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    text = normalize_arabic(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)  # remove non-Arabic\n",
        "    text = re.sub(r\"\\s+\", \" \", text)     # normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "# ğŸ—‚ï¸ ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† UCI\n",
        "\n",
        "# ØªÙ†Ø¸ÙŠÙ ÙˆØªØ·Ø¨ÙŠØ¹\n",
        "ds_ar[\"text\"] = ds_ar[\"text\"].fillna(\"\").apply(clean_arabic_text)\n",
        "\n",
        "ds_ar[\"label\"] =ds_ar[\"class\"].replace({\"Negative\": 0, \"Positive\": 1}).astype(int)\n",
        "\n",
        "\n",
        "# Ø§Ø®ØªÙŠØ§Ø± 1000 Ø¹ÙŠÙ†Ø©\n",
        "ds_ar = ds_ar.sample(50000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Dataset\n",
        "ds_ar = Dataset.from_pandas(ds_ar[[\"text\", \"label\"]])\n",
        "\n",
        "# 2ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ 1000 ØªØºØ±ÙŠØ¯Ø© Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù…Ù† Sentiment140 (Stanford)\n",
        "\n",
        "if os.path.exists(file_path_en_dataset):\n",
        "     print(\"âœ… Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯.\")\n",
        "     print(file_path_en_dataset)\n",
        "     print(\"-----\")\n",
        "else:\n",
        "      print(\"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.\")\n",
        "      !wget -q https://cs.stanford.edu/people/alecmgo/tra.iningandtestdata.zip -O sentiment140.zip\n",
        "      !unzip -o sentiment140.zip\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "cols = ['label', 'id', 'date', 'query', 'user', 'text']\n",
        "#ds_en = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', names=cols)\n",
        "\n",
        "ds_en = pd.read_csv(file_path_en_dataset, encoding='latin-1', names=cols)\n",
        "\n",
        "# 3. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:\n",
        "# 0 â†’ Ø³Ù„Ø¨ÙŠ | 4 â†’ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ â†’ Ù†Ø­ÙˆÙ„Ù‡Ø§ Ù„Ù€ 1\n",
        "ds_en = ds_en[ds_en['label'].isin([0, 4])]\n",
        "ds_en['label'] =ds_en['label'].map({0: 0, 4: 1})\n",
        "\n",
        "# 4. ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù… Ù„Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù„Ù„ØªØ¬Ø±Ø¨Ø©\n",
        "ds_en = ds_en.sample(50000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 5. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Dataset\n",
        "from datasets import Dataset\n",
        "ds_en = Dataset.from_pandas(ds_en[['text', 'label']])\n",
        "\n",
        "# 3ï¸âƒ£ Ø¯Ù…Ø¬ Ø§Ù„Ù„ØºØªÙŠÙ† ÙÙŠ Dataset ÙˆØ§Ø­Ø¯\n",
        "ds = concatenate_datasets([\n",
        "    ds_ar,\n",
        "    ds_en\n",
        "]).shuffle(seed=44)\n",
        "\n",
        "print(\"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª:\", len(ds))\n",
        "\n",
        "# 4ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Tokenizer Ùˆ Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "# 5ï¸âƒ£ ØªØ´ÙÙŠØ± Ø§Ù„Ù†ØµÙˆØµ\n",
        "def tok(x):\n",
        "    return tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "ds = ds.map(tok, batched=True)\n",
        "\n",
        "# 6ï¸âƒ£ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "split = ds.train_test_split(test_size=0.1, seed=45)\n",
        "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "# 7ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=-1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"sentiment_ar_en_model\",\n",
        "    eval_strategy=\"epoch\", # Changed back to evaluation_strategy\n",
        "    #evaluation_steps=50, # Added evaluation_steps\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 8ï¸âƒ£ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "trainer.train()\n",
        "\n",
        "# 9ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "trainer.save_model(\"sentiment_model_final\")\n",
        "tokenizer.save_pretrained(\"sentiment_model_final\")"
      ],
      "metadata": {
        "id": "SHGwYB4ZT1FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "hf_username = \"HatemMoushir\"\n",
        "repo_name = \"ArEn-TweetSentiment-BERT-Hatem\"\n",
        "\n",
        "\n",
        "upload_folder(\n",
        "    folder_path=\"/content/sentiment_model_final\",\n",
        "\n",
        "    repo_id=f\"{hf_username}/{repo_name}\",\n",
        "    repo_type=\"model\"\n",
        ")"
      ],
      "metadata": {
        "id": "iF52qBtjMphg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"HatemMoushir/ArEn-TweetSentiment-BERT-Hatem\")\n",
        "\n",
        "# 100 Ø¬Ù…Ù„Ø© Ù…Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© (1 = Ø¥ÙŠØ¬Ø§Ø¨ÙŠØŒ 0 = Ø³Ù„Ø¨ÙŠ)\n",
        "samples = [\n",
        "    (\"Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ù‹Ø§ Ø§Ù„ÙŠÙˆÙ…\", 1),\n",
        "    (\"Ø§Ù„Ø¬Ùˆ Ù…Ù…Ø·Ø± ÙˆÙ‡Ø°Ø§ ÙŠØ¬Ø¹Ù„Ù†ÙŠ Ø­Ø²ÙŠÙ†Ù‹Ø§\", 0),\n",
        "    (\"Ù†Ø¬Ø­Øª ÙÙŠ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†!\", 1),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø¥Ø­Ø¨Ø§Ø· Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±\", 0),\n",
        "    (\"Ø£Ø­Ø¨ Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ ÙƒØ«ÙŠØ±Ù‹Ø§\", 1),\n",
        "    (\"Ù‡Ø°Ø§ Ø£Ø³ÙˆØ£ ÙŠÙˆÙ… ÙÙŠ Ø­ÙŠØ§ØªÙŠ\", 0),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© ÙˆØ§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø©\", 1),\n",
        "    (\"Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ù†ÙˆÙ… Ø¬ÙŠØ¯Ù‹Ø§ Ø§Ù„Ù„ÙŠÙ„Ø©\", 0),\n",
        "    (\"Ø§Ù„ÙŠÙˆÙ… Ø¬Ù…ÙŠÙ„ ÙˆÙ…Ø´Ù…Ø³\", 1),\n",
        "    (\"ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ³ÙŠØ± Ø¨Ø´ÙƒÙ„ Ø®Ø§Ø·Ø¦\", 0),\n",
        "    (\"Ø£Ø­Ø¨ Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ø£ÙÙ„Ø§Ù… Ù…Ø¹ Ø¹Ø§Ø¦Ù„ØªÙŠ\", 1),\n",
        "    (\"ØªØ£Ø®Ø±Øª Ø¹Ù† Ø§Ù„Ø¹Ù…Ù„ ÙˆÙÙ‚Ø¯Øª Ù…Ø²Ø§Ø¬ÙŠ\", 0),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ù†Ø´Ø§Ø· ÙˆØ§Ù„Ø­ÙŠÙˆÙŠØ©\", 1),\n",
        "    (\"Ø§Ù„Ù…ÙƒØ§Ù† Ù…Ø²Ø¯Ø­Ù… ÙˆÙ„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„ØªØ­Ù…Ù„\", 0),\n",
        "    (\"Ù‚Ø¶ÙŠØª Ø¹Ø·Ù„Ø© Ø±Ø§Ø¦Ø¹Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø·Ø¦\", 1),\n",
        "    (\"Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙŠÙˆÙ… Ø¨Ø´ÙƒÙ„ Ø³ÙŠØ¡\", 0),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„ØªÙØ§Ø¤Ù„ Ø¨Ø´Ø£Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„\", 1),\n",
        "    (\"Ù„Ù… ÙŠØ¹Ø¬Ø¨Ù†ÙŠ Ø§Ù„Ø·Ø¹Ø§Ù… Ø§Ù„ÙŠÙˆÙ…\", 0),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø­Ø¨ Ù…Ù† Ø§Ù„Ø¬Ù…ÙŠØ¹\", 1),\n",
        "    (\"Ø®Ø³Ø±Øª ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ù„Ø­Ø¸Ø©\", 0),\n",
        "    (\"Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ ØªØ¬Ø¹Ù„Ù†ÙŠ Ø³Ø¹ÙŠØ¯Ù‹Ø§\", 1),\n",
        "    (\"Ø§Ù„Ø·Ø±ÙŠÙ‚ Ù…Ø²Ø¯Ø­Ù… ÙˆØ£Ù†Ø§ ØºØ§Ø¶Ø¨\", 0),\n",
        "    (\"Ø£Ù†Ø§ Ù…Ù…ØªÙ† Ù„ÙƒÙ„ Ø´ÙŠØ¡ Ù„Ø¯ÙŠ\", 1),\n",
        "    (\"ÙƒØ§Ù† ÙŠÙˆÙ…Ù‹Ø§ Ù…Ø±Ù‡Ù‚Ù‹Ø§ Ø¬Ø¯Ù‹Ø§\", 0),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø£Ù…Ù„ Ø±ØºÙ… Ø§Ù„ØµØ¹ÙˆØ¨Ø§Øª\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ø·ÙŠÙ‚ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ø²ÙŠØ§Ø±Ø© Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ\", 1),\n",
        "    (\"ØªØ¬Ø§Ù‡Ù„Ù†ÙŠ ÙÙŠ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ ÙˆØ´Ø¹Ø±Øª Ø¨Ø§Ù„Ø¥Ù‡Ø§Ù†Ø©\", 0),\n",
        "    (\"ÙØ²Øª ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©!\", 1),\n",
        "    (\"Ø§Ù„Ø¬Ùˆ Ø®Ø§Ù†Ù‚ ÙˆÙ„Ø§ ÙŠÙØ­ØªÙ…Ù„\", 0),\n",
        "    (\"ØªÙ„Ù‚ÙŠØª Ø±Ø³Ø§Ù„Ø© Ø¬Ù…ÙŠÙ„Ø© Ù…Ù† ØµØ¯ÙŠÙ‚ÙŠ\", 1),\n",
        "    (\"Ø§Ù†Ù‚Ø·Ø¹Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ÙˆÙØ§ØªÙ†ÙŠ Ø§Ù„ÙÙŠÙ„Ù…\", 0),\n",
        "    (\"Ø£Ù†Ø§ Ù…Ø­Ø¸ÙˆØ¸ Ø¨Ø¹Ø§Ø¦Ù„ØªÙŠ\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ø­Ø¯ ÙŠÙ‡ØªÙ… Ø¨ÙŠ\", 0),\n",
        "    (\"Ø§Ù„Ù‡Ø¯ÙˆØ¡ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙƒØ§Ù† ÙŠØ±ÙŠØ­Ù†ÙŠ\", 1),\n",
        "    (\"Ø®Ø³Ø±Øª ÙØ±ØµØªÙŠ Ø§Ù„Ø£Ø®ÙŠØ±Ø©\", 0),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø£Ù†Ù†ÙŠ Ù…Ø­Ø¨ÙˆØ¨\", 1),\n",
        "    (\"Ø¶Ø§Ø¹Øª Ø£Ù…ØªØ¹ØªÙŠ ÙÙŠ Ø§Ù„Ù…Ø·Ø§Ø±\", 0),\n",
        "    (\"Ù‚Ù…Øª Ø¨Ø¹Ù…Ù„ Ø¬ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ…\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ø±ÙŠØ¯ Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø¹ Ø£Ø­Ø¯\", 0),\n",
        "    (\"Ø£Ù†Ø§ Ù…Ù…ØªÙ† Ù„Ù„Ø­ÙŠØ§Ø©\", 1),\n",
        "    (\"ÙŠÙˆÙ… Ù…Ù…Ù„ ÙˆØ¨Ù„Ø§ ÙØ§Ø¦Ø¯Ø©\", 0),\n",
        "    (\"ØªÙ„Ù‚ÙŠØª ØªØ±Ù‚ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ù…Ù„\", 1),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø¥Ø¬Ù‡Ø§Ø¯ ÙˆØ§Ù„ØªØ¹Ø¨\", 0),\n",
        "    (\"Ø§Ù„Ù‡Ø¯ÙŠØ© Ø£Ø³Ø¹Ø¯ØªÙ†ÙŠ ÙƒØ«ÙŠØ±Ù‹Ø§\", 1),\n",
        "    (\"Ø§Ù†Ù‡Ø±Øª Ù…Ù† Ø§Ù„Ø¶ØºØ·\", 0),\n",
        "    (\"ØªÙ†Ø§ÙˆÙ„Øª ÙˆØ¬Ø¨Ø© Ù„Ø°ÙŠØ°Ø©\", 1),\n",
        "    (\"ØªØ£Ø®Ø±Øª Ø§Ù„Ø±Ø­Ù„Ø© ÙˆØ£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø¶ÙŠÙ‚\", 0),\n",
        "    (\"Ø­Ù‚Ù‚Øª Ù‡Ø¯ÙÙ‹Ø§ ÙƒÙ†Øª Ø£Ø³Ø¹Ù‰ Ù„Ù‡\", 1),\n",
        "    (\"Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙƒØ§Ù†Øª Ù‚Ø§Ø³ÙŠØ©\", 0),\n",
        "    (\"Ø£Ù†Ø§ ÙØ®ÙˆØ± Ø¨Ù†ÙØ³ÙŠ\", 1),\n",
        "    (\"ÙÙ‚Ø¯Øª Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ù…Ù† Ø­ÙˆÙ„ÙŠ\", 0),\n",
        "    (\"Ø¹Ø·Ù„Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ ÙƒØ§Ù†Øª Ø±Ø§Ø¦Ø¹Ø©\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ø¬Ø¯ Ø£ÙŠ Ø¯Ø§ÙØ¹ Ù„Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±\", 0),\n",
        "    (\"Ø§Ø¨Ù†ÙŠ Ù†Ø¬Ø­ ÙÙŠ Ø¯Ø±Ø§Ø³ØªÙ‡\", 1),\n",
        "    (\"ÙƒÙ„ Ù…Ù† Ø­ÙˆÙ„ÙŠ Ø®Ø°Ù„Ù†ÙŠ\", 0),\n",
        "    (\"Ù…Ø´ÙŠØª Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø­Ø± ÙˆÙƒØ§Ù† Ø§Ù„Ø¬Ùˆ Ø¬Ù…ÙŠÙ„Ù‹Ø§\", 1),\n",
        "    (\"ØªØ¹Ø±Ø¶Øª Ù„Ù…ÙˆÙ‚Ù Ù…Ø­Ø±Ø¬ Ø£Ù…Ø§Ù… Ø§Ù„Ø¬Ù…ÙŠØ¹\", 0),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© Ù„Ø£Ù†ÙŠ Ø³Ø§Ø¹Ø¯Øª Ø´Ø®ØµÙ‹Ø§\", 1),\n",
        "    (\"ØªÙ… ØªØ¬Ø§Ù‡Ù„ÙŠ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„\", 0),\n",
        "    (\"Ù†Ù…Øª Ø¬ÙŠØ¯Ù‹Ø§ ÙˆØ§Ø³ØªÙŠÙ‚Ø¸Øª Ø¨Ù†Ø´Ø§Ø·\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ø´Ø¹Ø± Ø¨Ø£ÙŠ ØªÙ‚Ø¯Ù…\", 0),\n",
        "    (\"ÙŠÙˆÙ… Ø±Ø§Ø¦Ø¹ Ù…Ø¹ Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ\", 1),\n",
        "    (\"ÙØ´Ù„Øª Ù…Ø±Ø© Ø£Ø®Ø±Ù‰\", 0),\n",
        "    (\"ØªÙ„Ù‚ÙŠØª Ù…ÙƒØ§Ù„Ù…Ø© Ø£Ø³Ø¹Ø¯ØªÙ†ÙŠ\", 1),\n",
        "    (\"ÙƒÙ„ Ø´ÙŠØ¡ ÙŠÙ†Ù‡Ø§Ø± Ù…Ù† Ø­ÙˆÙ„ÙŠ\", 0),\n",
        "    (\"Ø§Ø³ØªÙ…ØªØ¹Øª Ø¨Ø§Ù„Ø£Ø¬ÙˆØ§Ø¡ Ø§Ù„ÙŠÙˆÙ…\", 1),\n",
        "    (\"Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ù‚Ù„Ù‚ Ø§Ù„Ù…Ø³ØªÙ…Ø±\", 0),\n",
        "    (\"ÙƒØ§Ù† Ø§Ù„Ù„Ù‚Ø§Ø¡ Ø¯Ø§ÙØ¦Ù‹Ø§ ÙˆÙ…Ù„ÙŠØ¦Ù‹Ø§ Ø¨Ø§Ù„Ø­Ø¨\", 1),\n",
        "    (\"Ù„Ø§ Ø£ØªØ­Ù…Ù„ Ø§Ù„Ø¶ØºØ· Ø£ÙƒØ«Ø±\", 0),\n",
        "    (\"Ù†Ø¬Ø­ Ù…Ø´Ø±ÙˆØ¹ÙŠ Ø£Ø®ÙŠØ±Ù‹Ø§\", 1),\n",
        "    (\"ÙÙ‚Ø¯Øª Ø¹Ù…Ù„ÙŠ Ø§Ù„ÙŠÙˆÙ…\", 0),\n",
        "    (\"Ù‚Ø¶ÙŠØª ÙˆÙ‚ØªÙ‹Ø§ Ù…Ù…ØªØ¹Ù‹Ø§ ÙÙŠ Ø§Ù„Ø­Ø¯ÙŠÙ‚Ø©\", 1),\n",
        "    (\"Ø£Ù†Ø§ Ø®Ø§Ø¦Ù Ù…Ù…Ø§ Ø³ÙŠØ£ØªÙŠ\", 0),\n",
        "    (\"ØªÙ„Ù‚ÙŠØª Ø¯Ø¹Ù…Ù‹Ø§ ÙƒØ¨ÙŠØ±Ù‹Ø§ Ù…Ù† Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ\", 1),\n",
        "    (\"Ø§Ù„ÙŠØ£Ø³ ÙŠØ³ÙŠØ·Ø± Ø¹Ù„ÙŠ\", 0),\n",
        "    (\"Ø±Ø­Ù„ØªÙŠ ÙƒØ§Ù†Øª Ù…Ù„ÙŠØ¦Ø© Ø¨Ø§Ù„ÙØ±Ø­\", 1),\n",
        "    (\"Ù„Ø§ Ø´ÙŠØ¡ ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ù…Ø¤Ø®Ø±Ù‹Ø§\", 0),\n",
        "    (\"Ø£Ø­Ø¨Ø¨Øª Ø§Ù„ÙÙŠÙ„Ù… ÙƒØ«ÙŠØ±Ù‹Ø§\", 1),\n",
        "    (\"ÙƒÙ„Ù…Ø§ØªÙ‡Ù… Ø¬Ø±Ø­ØªÙ†ÙŠ\", 0),\n",
        "    (\"ØªØ°ÙˆÙ‚Øª Ø·Ø¹Ø§Ù…Ù‹Ø§ Ø±Ø§Ø¦Ø¹Ù‹Ø§\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ø±Ù‰ ÙØ§Ø¦Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©\", 0),\n",
        "    (\"Ø¶Ø­ÙƒÙ†Ø§ ÙƒØ«ÙŠØ±Ù‹Ø§ Ø§Ù„ÙŠÙˆÙ…\", 1),\n",
        "    (\"Ø­Ù„Ù…ÙŠ ØªØ¨Ø®Ø±\", 0),\n",
        "    (\"Ù„Ø­Ø¸Ø© Ø§Ù„Ù„Ù‚Ø§Ø¡ ÙƒØ§Ù†Øª Ø³Ø§Ø­Ø±Ø©\", 1),\n",
        "    (\"Ø®Ø³Ø±Øª Ø£Ù‚Ø±Ø¨ Ø§Ù„Ù†Ø§Ø³ Ø¥Ù„ÙŠ\", 0),\n",
        "    (\"Ø§Ù„Ù…Ø´ÙŠ ÙÙŠ Ø§Ù„Ø·Ø¨ÙŠØ¹Ø© ÙŠØ±ÙŠØ­ Ø£Ø¹ØµØ§Ø¨ÙŠ\", 1),\n",
        "    (\"Ù„Ù… ÙŠØµØ¯Ù‚Ù†ÙŠ Ø£Ø­Ø¯\", 0),\n",
        "    (\"Ø§Ø¨ØªØ³Ø§Ù…Ø© Ø·ÙÙ„ Ø¬Ø¹Ù„Øª ÙŠÙˆÙ…ÙŠ Ø£ÙØ¶Ù„\", 1),\n",
        "    (\"ÙƒÙ„ Ø´ÙŠØ¡ Ø£ØµØ¨Ø­ ØµØ¹Ø¨Ù‹Ø§\", 0),\n",
        "    (\"Ø§Ù„ÙŠÙˆÙ… Ø§Ø­ØªÙÙ„Øª Ø¨Ù†Ø¬Ø§Ø­ÙŠ\", 1),\n",
        "    (\"Ø§Ù†Ù‡Ø§Ø± ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ù„Ø­Ø¸Ø©\", 0),\n",
        "    (\"Ø£Ù…Ø¶ÙŠØª ÙˆÙ‚ØªÙ‹Ø§ Ù…Ù…ØªØ¹Ù‹Ø§ Ù…Ø¹ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø©\", 1),\n",
        "    (\"ÙÙ‚Ø¯Øª Ø§Ù„Ø£Ù…Ù„ ØªÙ…Ø§Ù…Ù‹Ø§\", 0),\n",
        "    (\"Ù‚Ø¶ÙŠØª ÙŠÙˆÙ…Ù‹Ø§ Ø±Ø§Ø¦Ø¹Ù‹Ø§ ÙÙŠ Ø§Ù„Ø±ÙŠÙ\", 1),\n",
        "    (\"Ø§Ù„Ù†Ø§Ø³ Ù„Ø§ ÙŠÙÙ‡Ù…ÙˆÙ†Ù†ÙŠ\", 0),\n",
        "    (\"Ø§Ø³ØªÙ…ØªØ¹Øª Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ ÙˆØ§Ù„Ù‡Ø¯ÙˆØ¡\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© Ø£Ø¨Ø¯Ù‹Ø§\", 0),\n",
        "    (\"Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡ Ø¬Ù„Ø¨ÙˆØ§ Ù„ÙŠ Ø§Ù„Ø³Ø¹Ø§Ø¯Ø©\", 1),\n",
        "    (\"ØªØ¹Ø¨Øª Ù…Ù† Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©\", 0),\n",
        "    (\"ÙƒÙ„ Ù„Ø­Ø¸Ø© ÙƒØ§Ù†Øª Ø±Ø§Ø¦Ø¹Ø©\", 1),\n",
        "    (\"ÙƒÙ„ Ø´ÙŠØ¡ ÙØ´Ù„\", 0),\n",
        "    (\"Ø§Ù„Ù†Ø¬Ø§Ø­ ÙƒØ§Ù† Ø«Ù…Ø±Ø© Ø¬Ù‡Ø¯ÙŠ\", 1),\n",
        "    (\"Ù„Ø§ Ø£Ù…Ù„Ùƒ Ø´ÙŠØ¦Ù‹Ø§ Ø£ÙØ±Ø­ Ø¨Ù‡\", 0)\n",
        "]\n",
        "\n",
        "# ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
        "correct = 0\n",
        "\n",
        "for i, (text, true_label) in enumerate(samples):\n",
        "    result = classifier(text)[0]\n",
        "\n",
        "    predicted_label = 1 if result[\"label\"] == (\"LABEL_1\") else 0\n",
        "    is_correct = predicted_label == true_label\n",
        "    correct += is_correct\n",
        "\n",
        "    print(f\"{i+1}. \\\"{text}\\\"\")\n",
        "    print(f\"   ğŸ” Model â†’ {predicted_label} | ğŸ¯ True â†’ {true_label} | {'âœ”ï¸ ØµØ­' if is_correct else 'âŒ ØºÙ„Ø·'}\\n\")\n",
        "\n",
        "# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©\n",
        "accuracy = correct / len(samples)\n",
        "print(f\"âœ… Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Fucsz0I8NqVQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}