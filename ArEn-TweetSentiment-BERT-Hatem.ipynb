{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH2Jxs7W9ZWKWK1OCRpL1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/Sentiment/blob/main/ArEn-TweetSentiment-BERT-Hatem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ تثبيت المكتبات المطلوبة\n",
        "!pip install -q datasets transformers evaluate pandas scikit-learn\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import evaluate\n",
        "import os\n",
        "\n",
        "file_path_ar_dataset = \"/content/Arabic Tweets Sentiment Classification 2024/Arabic_tweets_sentiment.csv\"\n",
        "file_path_en_dataset = \"/content/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "if os.path.exists(file_path_ar_dataset):\n",
        "     print(\"✅ الملف موجود.\")\n",
        "     print(file_path_ar_dataset)\n",
        "     print(\"-----\")else:\n",
        "     print(\"❌ الملف غير موجود.\")\n",
        "     !wget -q https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/m88gg52wp7-1.zip -O arabicdata1.zip\n",
        "     !unzip -o arabicdata1.zip\n",
        "\n",
        "\n",
        "# ✅ تحميل بيانات التغريدات العربية من UCI\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV with the correct separator and rename the column\n",
        "#ds_ar = pd.read_csv(\"/content/Arabic Tweets Sentiment Classification 2024/Arabic_tweets_sentiment.csv\", encoding='utf-8', sep='\\t')\n",
        "ds_ar = pd.read_csv(file_path_ar_dataset, encoding='utf-8', sep='\\t')\n",
        "\n",
        "ds_ar.rename(columns={'class': 'class'}, inplace=True)\n",
        "\n",
        "\n",
        "# فلترة السجلات التي تحتوي فقط على Positive أو Negative\n",
        "ds_ar = ds_ar[ds_ar[\"class\"].isin([\"Positive\", \"Negative\"])]\n",
        "\n",
        "\n",
        "# 🧼 تنظيف النصوص الإنجليزية\n",
        "def clean_english_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)     # remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)        # remove mentions\n",
        "    text = re.sub(r\"#\", \"\", text)           # remove hashtags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)     # remove punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)         # remove numbers\n",
        "    return text.lower().strip()\n",
        "\n",
        "# 🧼 تطبيع وتنظيف النصوص العربية\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    text = normalize_arabic(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)  # remove non-Arabic\n",
        "    text = re.sub(r\"\\s+\", \" \", text)     # normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "# 🗂️ تجهيز بيانات التغريدات العربية من UCI\n",
        "\n",
        "# تنظيف وتطبيع\n",
        "ds_ar[\"text\"] = ds_ar[\"text\"].fillna(\"\").apply(clean_arabic_text)\n",
        "\n",
        "ds_ar[\"label\"] =ds_ar[\"class\"].replace({\"Negative\": 0, \"Positive\": 1}).astype(int)\n",
        "\n",
        "\n",
        "# اختيار 1000 عينة\n",
        "ds_ar = ds_ar.sample(50000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# تحويل إلى Dataset\n",
        "ds_ar = Dataset.from_pandas(ds_ar[[\"text\", \"label\"]])\n",
        "\n",
        "# 2️⃣ تحميل 1000 تغريدة إنجليزية من Sentiment140 (Stanford)\n",
        "\n",
        "if os.path.exists(file_path_en_dataset):\n",
        "     print(\"✅ الملف موجود.\")\n",
        "     print(file_path_en_dataset)\n",
        "     print(\"-----\")\n",
        "else:\n",
        "      print(\"❌ الملف غير موجود.\")\n",
        "      !wget -q https://cs.stanford.edu/people/alecmgo/tra.iningandtestdata.zip -O sentiment140.zip\n",
        "      !unzip -o sentiment140.zip\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "cols = ['label', 'id', 'date', 'query', 'user', 'text']\n",
        "#ds_en = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', names=cols)\n",
        "\n",
        "ds_en = pd.read_csv(file_path_en_dataset, encoding='latin-1', names=cols)\n",
        "\n",
        "# 3. تحويل التصنيفات:\n",
        "# 0 → سلبي | 4 → إيجابي → نحولها لـ 1\n",
        "ds_en = ds_en[ds_en['label'].isin([0, 4])]\n",
        "ds_en['label'] =ds_en['label'].map({0: 0, 4: 1})\n",
        "\n",
        "# 4. تقليل الحجم لعينة صغيرة للتجربة\n",
        "ds_en = ds_en.sample(50000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 5. تحويل إلى Dataset\n",
        "from datasets import Dataset\n",
        "ds_en = Dataset.from_pandas(ds_en[['text', 'label']])\n",
        "\n",
        "# 3️⃣ دمج اللغتين في Dataset واحد\n",
        "ds = concatenate_datasets([\n",
        "    ds_ar,\n",
        "    ds_en\n",
        "]).shuffle(seed=44)\n",
        "\n",
        "print(\"✅ إجمالي العبارات:\", len(ds))\n",
        "\n",
        "# 4️⃣ إعداد Tokenizer و Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "# 5️⃣ تشفير النصوص\n",
        "def tok(x):\n",
        "    return tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "ds = ds.map(tok, batched=True)\n",
        "\n",
        "# 6️⃣ تقسيم البيانات\n",
        "split = ds.train_test_split(test_size=0.1, seed=45)\n",
        "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "# 7️⃣ إعداد التدريب\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=-1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"sentiment_ar_en_model\",\n",
        "    eval_strategy=\"epoch\", # Changed back to evaluation_strategy\n",
        "    #evaluation_steps=50, # Added evaluation_steps\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 8️⃣ بدء التدريب\n",
        "trainer.train()\n",
        "\n",
        "# 9️⃣ حفظ النموذج\n",
        "trainer.save_model(\"sentiment_model_final\")\n",
        "tokenizer.save_pretrained(\"sentiment_model_final\")"
      ],
      "metadata": {
        "id": "SHGwYB4ZT1FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "hf_username = \"HatemMoushir\"\n",
        "repo_name = \"ArEn-TweetSentiment-BERT-Hatem\"\n",
        "\n",
        "\n",
        "upload_folder(\n",
        "    folder_path=\"/content/sentiment_model_final\",\n",
        "\n",
        "    repo_id=f\"{hf_username}/{repo_name}\",\n",
        "    repo_type=\"model\"\n",
        ")"
      ],
      "metadata": {
        "id": "iF52qBtjMphg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# تحميل النموذج\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"HatemMoushir/ArEn-TweetSentiment-BERT-Hatem\")\n",
        "\n",
        "# 100 جملة مع التصنيفات الحقيقية (1 = إيجابي، 0 = سلبي)\n",
        "samples = [\n",
        "    (\"أنا سعيد جدًا اليوم\", 1),\n",
        "    (\"الجو ممطر وهذا يجعلني حزينًا\", 0),\n",
        "    (\"نجحت في الامتحان!\", 1),\n",
        "    (\"أشعر بالإحباط من الأخبار\", 0),\n",
        "    (\"أحب أصدقائي كثيرًا\", 1),\n",
        "    (\"هذا أسوأ يوم في حياتي\", 0),\n",
        "    (\"أشعر بالراحة والطمأنينة\", 1),\n",
        "    (\"لم أتمكن من النوم جيدًا الليلة\", 0),\n",
        "    (\"اليوم جميل ومشمس\", 1),\n",
        "    (\"كل شيء يسير بشكل خاطئ\", 0),\n",
        "    (\"أحب مشاهدة الأفلام مع عائلتي\", 1),\n",
        "    (\"تأخرت عن العمل وفقدت مزاجي\", 0),\n",
        "    (\"أشعر بالنشاط والحيوية\", 1),\n",
        "    (\"المكان مزدحم ولا أستطيع التحمل\", 0),\n",
        "    (\"قضيت عطلة رائعة على الشاطئ\", 1),\n",
        "    (\"انتهى اليوم بشكل سيء\", 0),\n",
        "    (\"أشعر بالتفاؤل بشأن المستقبل\", 1),\n",
        "    (\"لم يعجبني الطعام اليوم\", 0),\n",
        "    (\"أشعر بالحب من الجميع\", 1),\n",
        "    (\"خسرت كل شيء في لحظة\", 0),\n",
        "    (\"الموسيقى تجعلني سعيدًا\", 1),\n",
        "    (\"الطريق مزدحم وأنا غاضب\", 0),\n",
        "    (\"أنا ممتن لكل شيء لدي\", 1),\n",
        "    (\"كان يومًا مرهقًا جدًا\", 0),\n",
        "    (\"أشعر بالأمل رغم الصعوبات\", 1),\n",
        "    (\"لا أطيق الانتظار لزيارة أصدقائي\", 1),\n",
        "    (\"تجاهلني في الاجتماع وشعرت بالإهانة\", 0),\n",
        "    (\"فزت في المسابقة!\", 1),\n",
        "    (\"الجو خانق ولا يُحتمل\", 0),\n",
        "    (\"تلقيت رسالة جميلة من صديقي\", 1),\n",
        "    (\"انقطعت الكهرباء وفاتني الفيلم\", 0),\n",
        "    (\"أنا محظوظ بعائلتي\", 1),\n",
        "    (\"لا أحد يهتم بي\", 0),\n",
        "    (\"الهدوء في هذا المكان يريحني\", 1),\n",
        "    (\"خسرت فرصتي الأخيرة\", 0),\n",
        "    (\"أشعر أنني محبوب\", 1),\n",
        "    (\"ضاعت أمتعتي في المطار\", 0),\n",
        "    (\"قمت بعمل جيد اليوم\", 1),\n",
        "    (\"لا أريد التحدث مع أحد\", 0),\n",
        "    (\"أنا ممتن للحياة\", 1),\n",
        "    (\"يوم ممل وبلا فائدة\", 0),\n",
        "    (\"تلقيت ترقية في العمل\", 1),\n",
        "    (\"أشعر بالإجهاد والتعب\", 0),\n",
        "    (\"الهدية أسعدتني كثيرًا\", 1),\n",
        "    (\"انهرت من الضغط\", 0),\n",
        "    (\"تناولت وجبة لذيذة\", 1),\n",
        "    (\"تأخرت الرحلة وأشعر بالضيق\", 0),\n",
        "    (\"حققت هدفًا كنت أسعى له\", 1),\n",
        "    (\"الخسارة كانت قاسية\", 0),\n",
        "    (\"أنا فخور بنفسي\", 1),\n",
        "    (\"فقدت الثقة في من حولي\", 0),\n",
        "    (\"عطلة نهاية الأسبوع كانت رائعة\", 1),\n",
        "    (\"لا أجد أي دافع للاستمرار\", 0),\n",
        "    (\"ابني نجح في دراسته\", 1),\n",
        "    (\"كل من حولي خذلني\", 0),\n",
        "    (\"مشيت على البحر وكان الجو جميلًا\", 1),\n",
        "    (\"تعرضت لموقف محرج أمام الجميع\", 0),\n",
        "    (\"أشعر بالسعادة لأني ساعدت شخصًا\", 1),\n",
        "    (\"تم تجاهلي بالكامل\", 0),\n",
        "    (\"نمت جيدًا واستيقظت بنشاط\", 1),\n",
        "    (\"لا أشعر بأي تقدم\", 0),\n",
        "    (\"يوم رائع مع أصدقائي\", 1),\n",
        "    (\"فشلت مرة أخرى\", 0),\n",
        "    (\"تلقيت مكالمة أسعدتني\", 1),\n",
        "    (\"كل شيء ينهار من حولي\", 0),\n",
        "    (\"استمتعت بالأجواء اليوم\", 1),\n",
        "    (\"أشعر بالقلق المستمر\", 0),\n",
        "    (\"كان اللقاء دافئًا ومليئًا بالحب\", 1),\n",
        "    (\"لا أتحمل الضغط أكثر\", 0),\n",
        "    (\"نجح مشروعي أخيرًا\", 1),\n",
        "    (\"فقدت عملي اليوم\", 0),\n",
        "    (\"قضيت وقتًا ممتعًا في الحديقة\", 1),\n",
        "    (\"أنا خائف مما سيأتي\", 0),\n",
        "    (\"تلقيت دعمًا كبيرًا من أصدقائي\", 1),\n",
        "    (\"اليأس يسيطر علي\", 0),\n",
        "    (\"رحلتي كانت مليئة بالفرح\", 1),\n",
        "    (\"لا شيء يسعدني مؤخرًا\", 0),\n",
        "    (\"أحببت الفيلم كثيرًا\", 1),\n",
        "    (\"كلماتهم جرحتني\", 0),\n",
        "    (\"تذوقت طعامًا رائعًا\", 1),\n",
        "    (\"لا أرى فائدة من المحاولة\", 0),\n",
        "    (\"ضحكنا كثيرًا اليوم\", 1),\n",
        "    (\"حلمي تبخر\", 0),\n",
        "    (\"لحظة اللقاء كانت ساحرة\", 1),\n",
        "    (\"خسرت أقرب الناس إلي\", 0),\n",
        "    (\"المشي في الطبيعة يريح أعصابي\", 1),\n",
        "    (\"لم يصدقني أحد\", 0),\n",
        "    (\"ابتسامة طفل جعلت يومي أفضل\", 1),\n",
        "    (\"كل شيء أصبح صعبًا\", 0),\n",
        "    (\"اليوم احتفلت بنجاحي\", 1),\n",
        "    (\"انهار كل شيء في لحظة\", 0),\n",
        "    (\"أمضيت وقتًا ممتعًا مع العائلة\", 1),\n",
        "    (\"فقدت الأمل تمامًا\", 0),\n",
        "    (\"قضيت يومًا رائعًا في الريف\", 1),\n",
        "    (\"الناس لا يفهمونني\", 0),\n",
        "    (\"استمتعت بالموسيقى والهدوء\", 1),\n",
        "    (\"لا أشعر بالسعادة أبدًا\", 0),\n",
        "    (\"الأصدقاء جلبوا لي السعادة\", 1),\n",
        "    (\"تعبت من المحاولة\", 0),\n",
        "    (\"كل لحظة كانت رائعة\", 1),\n",
        "    (\"كل شيء فشل\", 0),\n",
        "    (\"النجاح كان ثمرة جهدي\", 1),\n",
        "    (\"لا أملك شيئًا أفرح به\", 0)\n",
        "]\n",
        "\n",
        "# تجربة النموذج ومقارنة النتيجة\n",
        "correct = 0\n",
        "\n",
        "for i, (text, true_label) in enumerate(samples):\n",
        "    result = classifier(text)[0]\n",
        "\n",
        "    predicted_label = 1 if result[\"label\"] == (\"LABEL_1\") else 0\n",
        "    is_correct = predicted_label == true_label\n",
        "    correct += is_correct\n",
        "\n",
        "    print(f\"{i+1}. \\\"{text}\\\"\")\n",
        "    print(f\"   🔍 Model → {predicted_label} | 🎯 True → {true_label} | {'✔️ صح' if is_correct else '❌ غلط'}\\n\")\n",
        "\n",
        "# حساب الدقة\n",
        "accuracy = correct / len(samples)\n",
        "print(f\"✅ Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Fucsz0I8NqVQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}