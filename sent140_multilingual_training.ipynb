{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu52pRG4U+/vY1vgw6IXNF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/Sentiment/blob/main/sent140_multilingual_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "!pip install -q datasets transformers evaluate pandas\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import evaluate\n",
        "\n",
        "# ğŸ§¼ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©\n",
        "def clean_english_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)     # remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)        # remove mentions\n",
        "    text = re.sub(r\"#\", \"\", text)           # remove hashtags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)     # remove punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)         # remove numbers\n",
        "    return text.lower().strip()\n",
        "\n",
        "# ğŸ§¼ ØªØ·Ø¨ÙŠØ¹ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
        "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
        "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
        "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
        "    return text\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    text = normalize_arabic(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)  # remove non-Arabic\n",
        "    text = re.sub(r\"\\s+\", \" \", text)     # normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "# 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ 1000 ØªØºØ±ÙŠØ¯Ø© Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† ArSenTDâ€‘LEV\n",
        "ds_ar = load_dataset(\"ramybaly/arsentd_lev\", split=\"train\")\n",
        "ds_ar = ds_ar.filter(lambda x: x[\"Sentiment\"] in [1, 2]).shuffle(seed=42).select(range(1000))\n",
        "ds_ar = ds_ar.map(lambda x: {\n",
        "    \"text\": clean_arabic_text(x[\"Tweet\"]),\n",
        "    \"label\": 0 if x[\"Sentiment\"] == 1 else 1\n",
        "})\n",
        "\n",
        "# 2ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ 1000 ØªØºØ±ÙŠØ¯Ø© Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù…Ù† Sentiment140 (Stanford)\n",
        "!wget -q https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip -O sentiment140.zip\n",
        "!unzip -o sentiment140.zip\n",
        "\n",
        "ds_en = load_dataset(\"sentiment140\", split=\"train\")\n",
        "ds_en = ds_en.filter(lambda x: x[\"sentiment\"] in [0, 4]).shuffle(seed=43).select(range(1000))\n",
        "ds_en = ds_en.map(lambda x: {\n",
        "    \"text\": clean_english_text(x[\"text\"]),\n",
        "    \"label\": 0 if x[\"sentiment\"] == 0 else 1\n",
        "})\n",
        "\n",
        "# 3ï¸âƒ£ Ø¯Ù…Ø¬ Ø§Ù„Ù„ØºØªÙŠÙ† ÙÙŠ Dataset ÙˆØ§Ø­Ø¯\n",
        "ds = concatenate_datasets([\n",
        "    ds_ar.remove_columns([col for col in ds_ar.column_names if col not in [\"text\", \"label\"]]),\n",
        "    ds_en.remove_columns([col for col in ds_en.column_names if col not in [\"text\", \"label\"]])\n",
        "]).shuffle(seed=44)\n",
        "\n",
        "print(\"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª:\", len(ds))\n",
        "\n",
        "# 4ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Tokenizer Ùˆ Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "# 5ï¸âƒ£ ØªØ´ÙÙŠØ± Ø§Ù„Ù†ØµÙˆØµ\n",
        "def tok(x):\n",
        "    return tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "ds = ds.map(tok, batched=True)\n",
        "\n",
        "# 6ï¸âƒ£ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "split = ds.train_test_split(test_size=0.1, seed=45)\n",
        "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "# 7ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return accuracy.compute(predictions=np.argmax(p.predictions, axis=-1), references=p.label_ids)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"sentiment_ar_en_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 8ï¸âƒ£ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "SHGwYB4ZT1FZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}